{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38543cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Fashion-MNIST dataset...\n",
      "\n",
      "Pretraining autoencoder...\n",
      "Pretrain Epoch [1/100], Recon Loss: 0.1063\n",
      "Pretrain Epoch [2/100], Recon Loss: 0.0823\n",
      "Pretrain Epoch [3/100], Recon Loss: 0.0687\n",
      "Pretrain Epoch [4/100], Recon Loss: 0.0583\n",
      "Pretrain Epoch [5/100], Recon Loss: 0.0485\n",
      "Pretrain Epoch [6/100], Recon Loss: 0.0447\n",
      "Pretrain Epoch [7/100], Recon Loss: 0.0428\n",
      "Pretrain Epoch [8/100], Recon Loss: 0.0411\n",
      "Pretrain Epoch [9/100], Recon Loss: 0.0401\n",
      "Pretrain Epoch [10/100], Recon Loss: 0.0391\n",
      "Pretrain Epoch [11/100], Recon Loss: 0.0387\n",
      "Pretrain Epoch [12/100], Recon Loss: 0.0376\n",
      "Pretrain Epoch [13/100], Recon Loss: 0.0374\n",
      "Pretrain Epoch [14/100], Recon Loss: 0.0368\n",
      "Pretrain Epoch [15/100], Recon Loss: 0.0361\n",
      "Pretrain Epoch [16/100], Recon Loss: 0.0356\n",
      "Pretrain Epoch [17/100], Recon Loss: 0.0350\n",
      "Pretrain Epoch [18/100], Recon Loss: 0.0343\n",
      "Pretrain Epoch [19/100], Recon Loss: 0.0339\n",
      "Pretrain Epoch [20/100], Recon Loss: 0.0335\n",
      "Pretrain Epoch [21/100], Recon Loss: 0.0332\n",
      "Pretrain Epoch [22/100], Recon Loss: 0.0324\n",
      "Pretrain Epoch [23/100], Recon Loss: 0.0322\n",
      "Pretrain Epoch [24/100], Recon Loss: 0.0320\n",
      "Pretrain Epoch [25/100], Recon Loss: 0.0316\n",
      "Pretrain Epoch [26/100], Recon Loss: 0.0312\n",
      "Pretrain Epoch [27/100], Recon Loss: 0.0312\n",
      "Pretrain Epoch [28/100], Recon Loss: 0.0307\n",
      "Pretrain Epoch [29/100], Recon Loss: 0.0304\n",
      "Pretrain Epoch [30/100], Recon Loss: 0.0302\n",
      "Pretrain Epoch [31/100], Recon Loss: 0.0297\n",
      "Pretrain Epoch [32/100], Recon Loss: 0.0295\n",
      "Pretrain Epoch [33/100], Recon Loss: 0.0294\n",
      "Pretrain Epoch [34/100], Recon Loss: 0.0292\n",
      "Pretrain Epoch [35/100], Recon Loss: 0.0300\n",
      "Pretrain Epoch [36/100], Recon Loss: 0.0289\n",
      "Pretrain Epoch [37/100], Recon Loss: 0.0287\n",
      "Pretrain Epoch [38/100], Recon Loss: 0.0286\n",
      "Pretrain Epoch [39/100], Recon Loss: 0.0283\n",
      "Pretrain Epoch [40/100], Recon Loss: 0.0283\n",
      "Pretrain Epoch [41/100], Recon Loss: 0.0281\n",
      "Pretrain Epoch [42/100], Recon Loss: 0.0281\n",
      "Pretrain Epoch [43/100], Recon Loss: 0.0280\n",
      "Pretrain Epoch [44/100], Recon Loss: 0.0278\n",
      "Pretrain Epoch [45/100], Recon Loss: 0.0274\n",
      "Pretrain Epoch [46/100], Recon Loss: 0.0276\n",
      "Pretrain Epoch [47/100], Recon Loss: 0.0272\n",
      "Pretrain Epoch [48/100], Recon Loss: 0.0272\n",
      "Pretrain Epoch [49/100], Recon Loss: 0.0271\n",
      "Pretrain Epoch [50/100], Recon Loss: 0.0271\n",
      "Pretrain Epoch [51/100], Recon Loss: 0.0270\n",
      "Pretrain Epoch [52/100], Recon Loss: 0.0270\n",
      "Pretrain Epoch [53/100], Recon Loss: 0.0268\n",
      "Pretrain Epoch [54/100], Recon Loss: 0.0265\n",
      "Pretrain Epoch [55/100], Recon Loss: 0.0267\n",
      "Pretrain Epoch [56/100], Recon Loss: 0.0264\n",
      "Pretrain Epoch [57/100], Recon Loss: 0.0266\n",
      "Pretrain Epoch [58/100], Recon Loss: 0.0264\n",
      "Pretrain Epoch [59/100], Recon Loss: 0.0263\n",
      "Pretrain Epoch [60/100], Recon Loss: 0.0266\n",
      "Pretrain Epoch [61/100], Recon Loss: 0.0266\n",
      "Pretrain Epoch [62/100], Recon Loss: 0.0262\n",
      "Pretrain Epoch [63/100], Recon Loss: 0.0263\n",
      "Pretrain Epoch [64/100], Recon Loss: 0.0263\n",
      "Pretrain Epoch [65/100], Recon Loss: 0.0262\n",
      "Pretrain Epoch [66/100], Recon Loss: 0.0263\n",
      "Pretrain Epoch [67/100], Recon Loss: 0.0262\n",
      "Pretrain Epoch [68/100], Recon Loss: 0.0260\n",
      "Pretrain Epoch [69/100], Recon Loss: 0.0260\n",
      "Pretrain Epoch [70/100], Recon Loss: 0.0259\n",
      "Pretrain Epoch [71/100], Recon Loss: 0.0257\n",
      "Pretrain Epoch [72/100], Recon Loss: 0.0259\n",
      "Pretrain Epoch [73/100], Recon Loss: 0.0263\n",
      "Pretrain Epoch [74/100], Recon Loss: 0.0259\n",
      "Pretrain Epoch [75/100], Recon Loss: 0.0256\n",
      "Pretrain Epoch [76/100], Recon Loss: 0.0258\n",
      "Pretrain Epoch [77/100], Recon Loss: 0.0256\n",
      "Pretrain Epoch [78/100], Recon Loss: 0.0257\n",
      "Pretrain Epoch [79/100], Recon Loss: 0.0258\n",
      "Pretrain Epoch [80/100], Recon Loss: 0.0256\n",
      "Pretrain Epoch [81/100], Recon Loss: 0.0256\n",
      "Pretrain Epoch [82/100], Recon Loss: 0.0256\n",
      "Pretrain Epoch [83/100], Recon Loss: 0.0267\n",
      "Pretrain Epoch [84/100], Recon Loss: 0.0252\n",
      "Pretrain Epoch [85/100], Recon Loss: 0.0256\n",
      "Pretrain Epoch [86/100], Recon Loss: 0.0251\n",
      "Pretrain Epoch [87/100], Recon Loss: 0.0250\n",
      "Pretrain Epoch [88/100], Recon Loss: 0.0248\n",
      "Pretrain Epoch [89/100], Recon Loss: 0.0249\n",
      "Pretrain Epoch [90/100], Recon Loss: 0.0248\n",
      "Pretrain Epoch [91/100], Recon Loss: 0.0247\n",
      "Pretrain Epoch [92/100], Recon Loss: 0.0250\n",
      "Pretrain Epoch [93/100], Recon Loss: 0.0248\n",
      "Pretrain Epoch [94/100], Recon Loss: 0.0248\n",
      "Pretrain Epoch [95/100], Recon Loss: 0.0248\n",
      "Pretrain Epoch [96/100], Recon Loss: 0.0254\n",
      "Pretrain Epoch [97/100], Recon Loss: 0.0248\n",
      "Pretrain Epoch [98/100], Recon Loss: 0.0244\n",
      "Pretrain Epoch [99/100], Recon Loss: 0.0244\n",
      "Pretrain Epoch [100/100], Recon Loss: 0.0243\n",
      "\n",
      "Pretraining with enhanced contrastive loss...\n",
      "Contrastive Epoch [1/50], Triplet Loss: 0.8568\n",
      "Contrastive Epoch [2/50], Triplet Loss: 0.8182\n",
      "Contrastive Epoch [3/50], Triplet Loss: 0.8275\n",
      "Contrastive Epoch [4/50], Triplet Loss: 0.8327\n",
      "Contrastive Epoch [5/50], Triplet Loss: 0.8301\n",
      "Contrastive Epoch [6/50], Triplet Loss: 0.8362\n",
      "Contrastive Epoch [7/50], Triplet Loss: 0.8630\n",
      "Contrastive Epoch [8/50], Triplet Loss: 0.8729\n",
      "Contrastive Epoch [9/50], Triplet Loss: 0.8717\n",
      "Contrastive Epoch [10/50], Triplet Loss: 0.8922\n",
      "Contrastive Epoch [11/50], Triplet Loss: 0.9038\n",
      "Contrastive Epoch [12/50], Triplet Loss: 0.9205\n",
      "Contrastive Epoch [13/50], Triplet Loss: 0.9173\n",
      "Contrastive Epoch [14/50], Triplet Loss: 0.9508\n",
      "Contrastive Epoch [15/50], Triplet Loss: 0.9668\n",
      "Contrastive Epoch [16/50], Triplet Loss: 0.9843\n",
      "Contrastive Epoch [17/50], Triplet Loss: 1.0048\n",
      "Contrastive Epoch [18/50], Triplet Loss: 1.0163\n",
      "Contrastive Epoch [19/50], Triplet Loss: 1.0452\n",
      "Contrastive Epoch [20/50], Triplet Loss: 1.0477\n",
      "Contrastive Epoch [21/50], Triplet Loss: 1.0666\n",
      "Contrastive Epoch [22/50], Triplet Loss: 1.0882\n",
      "Contrastive Epoch [23/50], Triplet Loss: 1.0983\n",
      "Contrastive Epoch [24/50], Triplet Loss: 1.1178\n",
      "Contrastive Epoch [25/50], Triplet Loss: 1.1418\n",
      "Contrastive Epoch [26/50], Triplet Loss: 1.1806\n",
      "Contrastive Epoch [27/50], Triplet Loss: 1.1774\n",
      "Contrastive Epoch [28/50], Triplet Loss: 1.2299\n",
      "Contrastive Epoch [29/50], Triplet Loss: 1.2386\n",
      "Contrastive Epoch [30/50], Triplet Loss: 1.2264\n",
      "Contrastive Epoch [31/50], Triplet Loss: 1.2720\n",
      "Contrastive Epoch [32/50], Triplet Loss: 1.2921\n",
      "Contrastive Epoch [33/50], Triplet Loss: 1.2984\n",
      "Contrastive Epoch [34/50], Triplet Loss: 1.3292\n",
      "Contrastive Epoch [35/50], Triplet Loss: 1.3815\n",
      "Contrastive Epoch [36/50], Triplet Loss: 1.3818\n",
      "Contrastive Epoch [37/50], Triplet Loss: 1.3886\n",
      "Contrastive Epoch [38/50], Triplet Loss: 1.4023\n",
      "Contrastive Epoch [39/50], Triplet Loss: 1.4221\n",
      "Contrastive Epoch [40/50], Triplet Loss: 1.4344\n",
      "Contrastive Epoch [41/50], Triplet Loss: 1.4716\n",
      "Contrastive Epoch [42/50], Triplet Loss: 1.4876\n",
      "Contrastive Epoch [43/50], Triplet Loss: 1.5516\n",
      "Contrastive Epoch [44/50], Triplet Loss: 1.5359\n",
      "Contrastive Epoch [45/50], Triplet Loss: 1.5957\n",
      "Contrastive Epoch [46/50], Triplet Loss: 1.5574\n",
      "Contrastive Epoch [47/50], Triplet Loss: 1.6201\n",
      "Contrastive Epoch [48/50], Triplet Loss: 1.6401\n",
      "Contrastive Epoch [49/50], Triplet Loss: 1.6496\n",
      "Contrastive Epoch [50/50], Triplet Loss: 1.6684\n",
      "\n",
      "Initializing cluster centers with KMeans...\n",
      "\n",
      "Training DEC with enhanced clustering and contrastive loss...\n",
      "Iter 0: Silhouette score = 0.7454\n",
      "Iter 0: Label change rate = 0.0059\n",
      "Iter 0: Contrastive Loss = 0.6311\n",
      "Iter 0: Loss = 0.0700\n",
      "Iter 5: Silhouette score = 0.9462\n",
      "Iter 5: Label change rate = 0.1572\n",
      "Iter 5: Loss = 0.0621\n",
      "Iter 10: Silhouette score = 0.9016\n",
      "Iter 10: Label change rate = 0.0005\n",
      "Iter 10: Loss = 0.0543\n",
      "Iter 15: Silhouette score = 0.8414\n",
      "Iter 15: Label change rate = 0.0172\n",
      "Iter 15: Loss = 0.0480\n",
      "Iter 20: Silhouette score = 0.6665\n",
      "Iter 20: Label change rate = 0.1857\n",
      "Iter 20: Loss = 0.0453\n",
      "Iter 25: Silhouette score = 0.6780\n",
      "Iter 25: Label change rate = 0.0038\n",
      "Iter 25: Loss = 0.0424\n",
      "Iter 30: Silhouette score = 0.5926\n",
      "Iter 30: Label change rate = 0.0959\n",
      "Iter 30: Contrastive Loss = 0.6978\n",
      "Iter 30: Loss = 0.0330\n",
      "Iter 35: Label change rate = 0.2014\n",
      "Iter 35: Loss = 0.0535\n",
      "Iter 40: Label change rate = 0.0000\n",
      "Iter 40: Loss = 0.0496\n",
      "Iter 45: Label change rate = 0.0000\n",
      "Iter 45: Loss = 0.0466\n",
      "Iter 50: Label change rate = 0.0000\n",
      "Iter 50: Loss = 0.0457\n",
      "Iter 55: Label change rate = 0.0000\n",
      "Converged.\n",
      "Loaded best model with silhouette score: 0.9462\n",
      "\n",
      "Final Evaluation:\n",
      "Latent space std: 0.0427\n",
      "DEC Clustering - Warning: Only one cluster found\n",
      "\n",
      "Trying different K-Means configurations for comparison...\n",
      "K-Means (k=5) - Silhouette: 0.8175, Davies-Bouldin: 0.2535\n",
      "K-Means (k=8) - Silhouette: 0.7801, Davies-Bouldin: 0.2760\n",
      "K-Means (k=10) - Silhouette: 0.7429, Davies-Bouldin: 0.4305\n",
      "K-Means (k=12) - Silhouette: 0.6920, Davies-Bouldin: 0.5715\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from minisom import MiniSom\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_fashion_mnist_dataset(max_samples=10000):\n",
    "    dataset = load_dataset(\"zalando-datasets/fashion_mnist\", split=\"train\")\n",
    "    images = np.array([np.array(img[\"image\"]).flatten() for img in dataset]) / 255.0\n",
    "    labels = np.array([img[\"label\"] for img in dataset]) \n",
    "    images = images[:max_samples]\n",
    "    labels = labels[:max_samples]\n",
    "    noise = np.random.normal(0, 0.02, images.shape)\n",
    "    images = np.clip(images + noise, 0, 1)\n",
    "    return torch.FloatTensor(images), torch.LongTensor(labels)\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anchor_img = self.images[idx]\n",
    "        anchor_label = self.labels[idx]\n",
    "        pos_indices = torch.where(self.labels == anchor_label)[0]\n",
    "        pos_indices = pos_indices[pos_indices != idx] \n",
    "        if len(pos_indices) == 0: \n",
    "            pos_idx = idx\n",
    "        else:\n",
    "            pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,))]\n",
    "        pos_img = self.images[pos_idx]\n",
    "        neg_indices = torch.where(self.labels != anchor_label)[0]\n",
    "        neg_idx = neg_indices[torch.randint(0, len(neg_indices), (1,))]\n",
    "        neg_img = self.images[neg_idx]\n",
    "        if len(neg_indices) > 50:\n",
    "            random_neg_indices = neg_indices[torch.randint(0, len(neg_indices), (50,))]\n",
    "            random_neg_samples = self.images[random_neg_indices]\n",
    "            distances = torch.sum((anchor_img.unsqueeze(0) - random_neg_samples) ** 2, dim=1)\n",
    "            hard_neg_idx = random_neg_indices[torch.argmin(distances)]\n",
    "            hard_neg_img = self.images[hard_neg_idx]\n",
    "            if torch.rand(1) > 0.5:\n",
    "                neg_img = hard_neg_img\n",
    "        \n",
    "        return anchor_img, pos_img, neg_img, anchor_label\n",
    "\n",
    "class ImprovedDEC(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim1=512, hidden_dim2=256, hidden_dim3=128, \n",
    "                 hidden_dim4=64, embed_dim=128, n_clusters=10, dropout_rate=0.2):\n",
    "        super(ImprovedDEC, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layer1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.encoder_layer2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.encoder_layer3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.encoder_layer4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.encoder_layer5 = nn.Linear(hidden_dim4, embed_dim)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorm1 = nn.LayerNorm(hidden_dim1)\n",
    "        self.layernorm2 = nn.LayerNorm(hidden_dim2)\n",
    "        self.layernorm3 = nn.LayerNorm(hidden_dim3)\n",
    "        self.layernorm4 = nn.LayerNorm(hidden_dim4)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_layer1 = nn.Linear(embed_dim, hidden_dim4)\n",
    "        self.decoder_layer2 = nn.Linear(hidden_dim4, hidden_dim3)\n",
    "        self.decoder_layer3 = nn.Linear(hidden_dim3, hidden_dim2)\n",
    "        self.decoder_layer4 = nn.Linear(hidden_dim2, hidden_dim1)\n",
    "        self.decoder_layer5 = nn.Linear(hidden_dim1, input_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.cluster_centers = nn.Parameter(torch.randn(n_clusters, embed_dim))\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x1 = F.relu(self.encoder_layer1(x))\n",
    "        x1 = self.layernorm1(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        x2 = F.relu(self.encoder_layer2(x1))\n",
    "        x2 = self.layernorm2(x2)\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        x3 = F.relu(self.encoder_layer3(x2))\n",
    "        x3 = self.layernorm3(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "        \n",
    "        x4 = F.relu(self.encoder_layer4(x3))\n",
    "        x4 = self.layernorm4(x4)\n",
    "        x4 = self.dropout(x4)\n",
    "        \n",
    "        z = F.relu(self.encoder_layer5(x4))\n",
    "        return z\n",
    "        \n",
    "    def decode(self, z):\n",
    "        x4 = F.relu(self.decoder_layer1(z))\n",
    "        x3 = F.relu(self.decoder_layer2(x4))\n",
    "        x2 = F.relu(self.decoder_layer3(x3))\n",
    "        x1 = F.relu(self.decoder_layer4(x2))\n",
    "        x_hat = torch.sigmoid(self.decoder_layer5(x1))\n",
    "        return x_hat\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        q = 1.0 / (1.0 + (torch.sum((z.unsqueeze(1) - self.cluster_centers)**2, dim=2) / 2))\n",
    "        q = q ** 2 \n",
    "        q = (q.t() / torch.sum(q, dim=1)).t()\n",
    "        \n",
    "        return x_hat, q, z\n",
    "\n",
    "def enhanced_triplet_loss(anchor, positive, negative, margin=1.0, beta=0.2):\n",
    "    pos_dist = torch.sum((anchor - positive) ** 2, dim=1)\n",
    "    neg_dist = torch.sum((anchor - negative) ** 2, dim=1)\n",
    "    scaled_pos_dist = (1 + beta) * pos_dist\n",
    "    scaled_neg_dist = (1 - beta) * neg_dist\n",
    "    base_loss = torch.clamp(scaled_pos_dist - scaled_neg_dist + margin, min=0.0)\n",
    "    weight = torch.exp(base_loss)\n",
    "    weighted_loss = base_loss * weight\n",
    "    \n",
    "    return torch.mean(weighted_loss) / torch.mean(weight)\n",
    "\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.t() / weight.sum(1)).t()\n",
    "\n",
    "def cluster_separation_loss(embeddings, cluster_centers, cluster_assignments):\n",
    "    batch_size = embeddings.size(0)\n",
    "    n_clusters = cluster_centers.size(0)\n",
    "    \n",
    "    assigned_centers = cluster_centers[cluster_assignments]\n",
    "    assigned_distances = torch.sum((embeddings - assigned_centers) ** 2, dim=1)\n",
    "    expanded_embeddings = embeddings.unsqueeze(1).expand(batch_size, n_clusters, -1)\n",
    "    expanded_centers = cluster_centers.unsqueeze(0).expand(batch_size, n_clusters, -1)\n",
    "    all_distances = torch.sum((expanded_embeddings - expanded_centers) ** 2, dim=2)\n",
    "    \n",
    "    mask = torch.ones_like(all_distances, device=embeddings.device)\n",
    "    mask.scatter_(1, cluster_assignments.unsqueeze(1), 0)\n",
    "    \n",
    "    wrong_distances = all_distances + (1 - mask) * 1e6\n",
    "    closest_wrong_distances = torch.min(wrong_distances, dim=1)[0]\n",
    "    margin = 2.0 \n",
    "    loss = torch.mean(assigned_distances - closest_wrong_distances + margin)\n",
    "    return torch.clamp(loss, min=0.0)\n",
    "\n",
    "def between_cluster_variance_loss(embeddings, cluster_assignments, n_clusters):\n",
    "    cluster_means = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_mask = (cluster_assignments == i)\n",
    "        if torch.sum(cluster_mask) > 0: \n",
    "            cluster_mean = torch.mean(embeddings[cluster_mask], dim=0)\n",
    "            cluster_means.append(cluster_mean)\n",
    "    \n",
    "    if len(cluster_means) <= 1:\n",
    "        return torch.tensor(0.0, device=embeddings.device)\n",
    "    \n",
    "    cluster_means = torch.stack(cluster_means)\n",
    "    n_valid_clusters = cluster_means.size(0)\n",
    "    expanded_means1 = cluster_means.unsqueeze(1).expand(n_valid_clusters, n_valid_clusters, -1)\n",
    "    expanded_means2 = cluster_means.unsqueeze(0).expand(n_valid_clusters, n_valid_clusters, -1)\n",
    "    pairwise_distances = torch.sum((expanded_means1 - expanded_means2) ** 2, dim=2)\n",
    "    \n",
    "    mask = 1.0 - torch.eye(n_valid_clusters, device=embeddings.device)\n",
    "    \n",
    "    between_variance = torch.sum(pairwise_distances * mask) / (n_valid_clusters * (n_valid_clusters - 1))\n",
    "    return -between_variance\n",
    "\n",
    "def main():\n",
    "    print(\"Loading Fashion-MNIST dataset...\")\n",
    "    data, labels = load_fashion_mnist_dataset(max_samples=10000)\n",
    "    model = ImprovedDEC(\n",
    "        input_dim=784, \n",
    "        hidden_dim1=512, \n",
    "        hidden_dim2=256, \n",
    "        hidden_dim3=128, \n",
    "        hidden_dim4=64, \n",
    "        embed_dim=128, \n",
    "        n_clusters=10,  \n",
    "        dropout_rate=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    standard_loader = DataLoader(TensorDataset(data), batch_size=128, shuffle=True)\n",
    "    contrastive_dataset = ContrastiveDataset(data, labels)\n",
    "    contrastive_loader = DataLoader(\n",
    "    contrastive_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    drop_last=True,\n",
    "    collate_fn=contrastive_collate_fn\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPretraining autoencoder...\")\n",
    "    model.train()\n",
    "    best_recon_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 10\n",
    "    \n",
    "    for epoch in range(100): \n",
    "        total_loss = 0\n",
    "        for batch in standard_loader:\n",
    "            batch_data = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, _, z = model(batch_data)\n",
    "            \n",
    "            recon_loss = F.mse_loss(x_hat, batch_data)\n",
    "            \n",
    "            l2_reg = 0.001 * torch.mean(torch.sum(z**2, dim=1))\n",
    "            \n",
    "            loss = recon_loss + l2_reg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(standard_loader)\n",
    "        print(f\"Pretrain Epoch [{epoch+1}/100], Recon Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if avg_loss < best_recon_loss:\n",
    "            best_recon_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        scheduler.step(avg_loss)\n",
    "    \n",
    "    print(\"\\nPretraining with enhanced contrastive loss...\")\n",
    "    for epoch in range(50): \n",
    "        total_contrastive_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for anchor_img, pos_img, neg_img, _ in contrastive_loader:\n",
    "            anchor_img = anchor_img.to(device)\n",
    "            pos_img = pos_img.to(device)\n",
    "            neg_img = neg_img.to(device)\n",
    "            \n",
    "            batch_size = anchor_img.size(0)\n",
    "            if batch_size <= 1: \n",
    "                continue\n",
    "            anchor_embed = model.encode(anchor_img)\n",
    "            pos_embed = model.encode(pos_img)\n",
    "            neg_embed = model.encode(neg_img)\n",
    "            \n",
    "            dynamic_margin = 1.0 + 0.02 * epoch \n",
    "            loss = enhanced_triplet_loss(\n",
    "                anchor_embed, pos_embed, neg_embed, \n",
    "                margin=dynamic_margin, \n",
    "                beta=0.1 + 0.01 * epoch \n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_contrastive_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "        if batch_count > 0:\n",
    "            print(f\"Contrastive Epoch [{epoch+1}/50], Triplet Loss: {total_contrastive_loss/batch_count:.4f}\")\n",
    "    \n",
    "    print(\"\\nInitializing cluster centers with KMeans...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z_init = model.encode(data).cpu().numpy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    z_init_scaled = scaler.fit_transform(z_init)\n",
    "    \n",
    "    best_score = -1\n",
    "    best_kmeans = None\n",
    "    for i in range(5): \n",
    "        kmeans = KMeans(n_clusters=model.n_clusters, n_init=10, random_state=42+i).fit(z_init_scaled)\n",
    "        if len(np.unique(kmeans.labels_)) > 1: \n",
    "            score = silhouette_score(z_init_scaled, kmeans.labels_)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_kmeans = kmeans\n",
    "    \n",
    "    if best_kmeans is None:\n",
    "        best_kmeans = KMeans(n_clusters=model.n_clusters, n_init=20, random_state=42).fit(z_init_scaled)\n",
    "    kmeans_centers_scaled = best_kmeans.cluster_centers_\n",
    "    if hasattr(scaler, 'scale_') and hasattr(scaler, 'mean_'):\n",
    "        kmeans_centers = kmeans_centers_scaled * scaler.scale_ + scaler.mean_\n",
    "    else:\n",
    "        kmeans_centers = kmeans_centers_scaled\n",
    "    model.cluster_centers.data = torch.tensor(kmeans_centers, dtype=torch.float32).to(device)\n",
    "    print(\"\\nTraining DEC with enhanced clustering and contrastive loss...\")\n",
    "    max_iter = 300  \n",
    "    update_interval = 5\n",
    "    tol = 5e-4 \n",
    "    y_pred_last = best_kmeans.labels_\n",
    "    best_silhouette = -1\n",
    "    best_model_state = None\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        if iteration % update_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                _, q_all, z_final = model(data)\n",
    "            p = target_distribution(q_all).detach()\n",
    "            y_pred = q_all.cpu().numpy().argmax(axis=1)\n",
    "            z_np = z_final.cpu().numpy()\n",
    "            \n",
    "            if len(np.unique(y_pred)) > 1:\n",
    "                current_silhouette = silhouette_score(z_np, y_pred)\n",
    "                print(f\"Iter {iteration}: Silhouette score = {current_silhouette:.4f}\")\n",
    "                \n",
    "                if current_silhouette > best_silhouette:\n",
    "                    best_silhouette = current_silhouette\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "            \n",
    "            delta = np.sum(y_pred != y_pred_last) / len(y_pred)\n",
    "            print(f\"Iter {iteration}: Label change rate = {delta:.4f}\")\n",
    "            if delta < tol and iteration > 50: \n",
    "                print(\"Converged.\")\n",
    "                break\n",
    "            y_pred_last = y_pred\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in standard_loader:\n",
    "            batch_data = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, q_batch, z_batch = model(batch_data)\n",
    "            \n",
    "            p_batch = p[:batch_data.size(0)]\n",
    "            cluster_assignments = torch.argmax(q_batch, dim=1)\n",
    "            recon_loss = F.mse_loss(x_hat, batch_data)\n",
    "            kl_loss = F.kl_div(q_batch.log(), p_batch.to(device), reduction='batchmean')\n",
    "            \n",
    "            sep_loss = cluster_separation_loss(z_batch, model.cluster_centers, cluster_assignments)\n",
    "            \n",
    "            bv_loss = between_cluster_variance_loss(z_batch, cluster_assignments, model.n_clusters)\n",
    "            \n",
    "            clustering_weight = min(0.01 * (iteration / 30), 0.1)  \n",
    "            sep_weight = min(0.005 * (iteration / 50), 0.05)\n",
    "            bv_weight = min(0.005 * (iteration / 50), 0.05)\n",
    "            \n",
    "            loss = recon_loss + clustering_weight * kl_loss + sep_weight * sep_loss + bv_weight * bv_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if iteration % 3 == 0:\n",
    "            contrastive_total_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for anchor_img, pos_img, neg_img, _ in contrastive_loader:\n",
    "                anchor_img = anchor_img.to(device)\n",
    "                pos_img = pos_img.to(device)\n",
    "                neg_img = neg_img.to(device)\n",
    "                \n",
    "                batch_size = anchor_img.size(0)\n",
    "                if batch_size <= 1:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                anchor_embed = model.encode(anchor_img)\n",
    "                pos_embed = model.encode(pos_img)\n",
    "                neg_embed = model.encode(neg_img)\n",
    "                \n",
    "                dynamic_margin = 1.0 + 0.01 * iteration / 10\n",
    "                contr_loss = enhanced_triplet_loss(\n",
    "                    anchor_embed, pos_embed, neg_embed, \n",
    "                    margin=dynamic_margin,\n",
    "                    beta=0.2\n",
    "                )\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                contr_loss.backward()\n",
    "                optimizer.step()\n",
    "                contrastive_total_loss += contr_loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "            if batch_count > 0 and iteration % 10 == 0:\n",
    "                print(f\"Iter {iteration}: Contrastive Loss = {contrastive_total_loss/batch_count:.4f}\")\n",
    "            \n",
    "        if iteration % update_interval == 0:\n",
    "            print(f\"Iter {iteration}: Loss = {total_loss/len(standard_loader):.4f}\")\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model with silhouette score: {best_silhouette:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"improved_dec_model.pt\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, q_final, z_final = model(data)\n",
    "        cluster_labels = q_final.cpu().numpy().argmax(axis=1)\n",
    "        z_np = z_final.cpu().numpy()\n",
    "    \n",
    "    print(f\"\\nFinal Evaluation:\")\n",
    "    print(f\"Latent space std: {z_np.std(axis=0).mean():.4f}\")\n",
    "    \n",
    "    if len(np.unique(cluster_labels)) > 1:\n",
    "        final_silhouette = silhouette_score(z_np, cluster_labels)\n",
    "        final_db = davies_bouldin_score(z_np, cluster_labels)\n",
    "        print(f\"DEC Clustering - Silhouette Score: {final_silhouette:.4f}, Davies-Bouldin Index: {final_db:.4f}\")\n",
    "    else:\n",
    "        print(\"DEC Clustering - Warning: Only one cluster found\")\n",
    "\n",
    "    visualize_clusters(z_np, cluster_labels, \n",
    "                      \"Improved DEC Clusters (t-SNE)\", \n",
    "                      \"improved_dec_fashion_mnist_clusters.png\",\n",
    "                      sample_size=2000)  \n",
    "    \n",
    "    print(\"\\nTrying different K-Means configurations for comparison...\")\n",
    "    for n_clusters in [5, 8, 10, 12]:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=20, random_state=42).fit(z_np)\n",
    "        kmeans_labels = kmeans.labels_\n",
    "        if len(np.unique(kmeans_labels)) > 1:\n",
    "            sil_score = silhouette_score(z_np, kmeans_labels)\n",
    "            db_score = davies_bouldin_score(z_np, kmeans_labels)\n",
    "            print(f\"K-Means (k={n_clusters}) - Silhouette: {sil_score:.4f}, Davies-Bouldin: {db_score:.4f}\")\n",
    "            \n",
    "            if n_clusters == 10:  \n",
    "                visualize_clusters(z_np, kmeans_labels, \n",
    "                                  f\"K-Means (k={n_clusters}) Clusters\", \n",
    "                                  f\"kmeans_{n_clusters}_fashion_mnist_clusters.png\",\n",
    "                                  sample_size=2000)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def contrastive_collate_fn(batch):\n",
    "    anchors = []\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    labels = []\n",
    "    \n",
    "    for anchor, pos, neg, label in batch:\n",
    "        anchors.append(anchor.view(-1))\n",
    "        positives.append(pos.view(-1))\n",
    "        negatives.append(neg.view(-1))\n",
    "        labels.append(label)\n",
    "    \n",
    "    anchors = torch.stack(anchors)\n",
    "    positives = torch.stack(positives)\n",
    "    negatives = torch.stack(negatives)\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return anchors, positives, negatives, labels\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anchor_img = self.images[idx].view(-1) \n",
    "        anchor_label = self.labels[idx]\n",
    "        pos_indices = torch.where(self.labels == anchor_label)[0]\n",
    "        pos_indices = pos_indices[pos_indices != idx] \n",
    "        if len(pos_indices) == 0: \n",
    "            pos_idx = idx\n",
    "        else:\n",
    "            pos_idx = pos_indices[torch.randint(0, len(pos_indices), (1,))]\n",
    "        pos_img = self.images[pos_idx].view(-1)  \n",
    "        neg_indices = torch.where(self.labels != anchor_label)[0]\n",
    "        neg_idx = neg_indices[torch.randint(0, len(neg_indices), (1,))]\n",
    "        neg_img = self.images[neg_idx].view(-1) \n",
    "        \n",
    "        if len(neg_indices) > 50: \n",
    "            random_neg_indices = neg_indices[torch.randint(0, len(neg_indices), (50,))]\n",
    "            random_neg_samples = self.images[random_neg_indices]\n",
    "            distances = torch.sum((anchor_img.unsqueeze(0) - random_neg_samples.view(len(random_neg_samples), -1)) ** 2, dim=1)\n",
    "            hard_neg_idx = random_neg_indices[torch.argmin(distances)]\n",
    "            hard_neg_img = self.images[hard_neg_idx].view(-1) \n",
    "            if torch.rand(1) > 0.5:\n",
    "                neg_img = hard_neg_img\n",
    "        \n",
    "        return anchor_img, pos_img, neg_img, anchor_label\n",
    "\n",
    "def create_dataloaders(data, labels):\n",
    "    standard_loader = DataLoader(TensorDataset(data), batch_size=128, shuffle=True)\n",
    "    \n",
    "    contrastive_dataset = ContrastiveDataset(data, labels)\n",
    "    contrastive_loader = DataLoader(\n",
    "        contrastive_dataset, \n",
    "        batch_size=64, \n",
    "        shuffle=True, \n",
    "        drop_last=True,\n",
    "        collate_fn=contrastive_collate_fn\n",
    "    )\n",
    "    \n",
    "    return standard_loader, contrastive_loader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
